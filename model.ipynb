{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":128754,"databundleVersionId":15439142,"sourceType":"competition"}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =========================================================\n# Predict the Flood â€” FULL Kaggle Notebook (Fixed RMSE)\n# Competition folder: /kaggle/input/predict-the-floodd\n# Target: FloodProbability (0..1)\n# Model: LightGBM CV + Safe Features + Optional CatBoost Blend\n# Output: submission.csv\n#\n# FIX: sklearn mean_squared_error no longer supports squared=False\n#      -> use root_mean_squared_error OR sqrt(MSE)\n# =========================================================\n\nimport os, gc, random, warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import StratifiedKFold\n\n# âœ… FIXED RMSE (works even if sklearn removed squared= parameter)\nfrom sklearn.metrics import mean_squared_error\ndef rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\nimport lightgbm as lgb\n\n# -----------------------------\n# Reproducibility\n# -----------------------------\nSEED = 42\ndef seed_everything(seed=SEED):\n    random.seed(seed)\n    np.random.seed(seed)\nseed_everything(SEED)\n\n# =========================================================\n# 1) Load Data (Kaggle paths)\n# =========================================================\nDATA_DIR = \"/kaggle/input/predict-the-floodd/\"\n\ntrain = pd.read_csv(f\"{DATA_DIR}/train.csv\")\ntest  = pd.read_csv(f\"{DATA_DIR}/test.csv\")\nsub   = pd.read_csv(f\"{DATA_DIR}/sample_submission.csv\")\n\nprint(\"train:\", train.shape, \"| test:\", test.shape, \"| sub:\", sub.shape)\ndisplay(train.head())\n\n# =========================================================\n# 2) Target and ID\n# =========================================================\nTARGET = \"FloodProbability\"\nassert TARGET in train.columns, f\"TARGET '{TARGET}' not found in train columns!\"\n\nID_COL = None\nfor c in train.columns:\n    if c.lower() in [\"id\", \"index\"]:\n        ID_COL = c\n        break\nprint(\"ID_COL:\", ID_COL)\n\n# =========================================================\n# 3) Split features/target + align columns\n# =========================================================\nX = train.drop(columns=[TARGET]).copy()\ny = train[TARGET].values\n\n# Ensure test has same feature columns\ntest = test[X.columns].copy()\n\nprint(\"\\nMissing values:\")\nprint(\"Train missing:\", X.isna().sum().sum())\nprint(\"Test missing :\", test.isna().sum().sum())\n\nprint(\"\\nTarget stats:\")\nprint(pd.Series(y).describe())\n\n# =========================================================\n# 4) Safe Feature Engineering (row-wise stats)\n# =========================================================\ndef add_row_stats(df):\n    df = df.copy()\n    num = df.select_dtypes(include=[np.number])\n\n    df[\"row_mean\"] = num.mean(axis=1)\n    df[\"row_std\"]  = num.std(axis=1)\n    df[\"row_min\"]  = num.min(axis=1)\n    df[\"row_max\"]  = num.max(axis=1)\n    df[\"row_sum\"]  = num.sum(axis=1)\n    return df\n\nX_fe = add_row_stats(X)\ntest_fe = add_row_stats(test)\n\nprint(\"\\nAfter FE:\")\nprint(\"X_fe:\", X_fe.shape, \"test_fe:\", test_fe.shape)\n\n# =========================================================\n# 5) CV Strategy: StratifiedKFold on binned target\n# =========================================================\nN_SPLITS = 5\nbins = pd.qcut(train[TARGET], q=20, labels=False, duplicates=\"drop\")\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n\n# =========================================================\n# 6) LightGBM CV Training\n# =========================================================\nlgb_params = {\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n    \"boosting_type\": \"gbdt\",\n    \"learning_rate\": 0.03,\n    \"num_leaves\": 256,\n    \"min_data_in_leaf\": 80,\n    \"feature_fraction\": 0.85,\n    \"bagging_fraction\": 0.85,\n    \"bagging_freq\": 1,\n    \"lambda_l1\": 0.0,\n    \"lambda_l2\": 2.0,\n    \"verbosity\": -1,\n    \"seed\": SEED,\n}\n\noof_lgb = np.zeros(len(train))\npred_test_lgb = np.zeros(len(test_fe))\nfold_scores_lgb = []\n\nprint(\"\\n===== LightGBM CV =====\")\nfor fold, (tr_idx, va_idx) in enumerate(skf.split(X_fe, bins), 1):\n    X_tr, X_va = X_fe.iloc[tr_idx], X_fe.iloc[va_idx]\n    y_tr, y_va = y[tr_idx], y[va_idx]\n\n    dtr = lgb.Dataset(X_tr, label=y_tr)\n    dva = lgb.Dataset(X_va, label=y_va)\n\n    model = lgb.train(\n        lgb_params,\n        dtr,\n        num_boost_round=20000,\n        valid_sets=[dva],\n        valid_names=[\"valid\"],\n        callbacks=[\n            lgb.early_stopping(stopping_rounds=500, verbose=False),\n            lgb.log_evaluation(period=500),\n        ],\n    )\n\n    va_pred = model.predict(X_va, num_iteration=model.best_iteration)\n    oof_lgb[va_idx] = va_pred\n\n    fold_rmse = rmse(y_va, va_pred)\n    fold_scores_lgb.append(fold_rmse)\n\n    pred_test_lgb += model.predict(test_fe, num_iteration=model.best_iteration) / N_SPLITS\n\n    print(f\"Fold {fold}: RMSE={fold_rmse:.6f} | best_iter={model.best_iteration}\")\n\n    del model, dtr, dva, X_tr, X_va, y_tr, y_va\n    gc.collect()\n\ncv_lgb = rmse(y, oof_lgb)\nprint(\"\\n[LGB] CV RMSE:\", cv_lgb)\nprint(\"[LGB] Fold scores:\", [round(s, 6) for s in fold_scores_lgb],\n      \"Mean:\", np.mean(fold_scores_lgb), \"Std:\", np.std(fold_scores_lgb))\n\n# =========================================================\n# 7) Optional CatBoost + Blend (often boosts LB)\n# =========================================================\nUSE_CATBOOST = True   # set False if you want only LGBM\n\nfinal_pred = pred_test_lgb.copy()\n\nif USE_CATBOOST:\n    try:\n        from catboost import CatBoostRegressor\n        cat_ok = True\n    except:\n        cat_ok = False\n        print(\"\\nCatBoost not available. Using LGB only.\")\n\n    if cat_ok:\n        print(\"\\n===== CatBoost CV =====\")\n        pred_test_cb = np.zeros(len(test_fe))\n        fold_scores_cb = []\n\n        for fold, (tr_idx, va_idx) in enumerate(skf.split(X_fe, bins), 1):\n            X_tr, X_va = X_fe.iloc[tr_idx], X_fe.iloc[va_idx]\n            y_tr, y_va = y[tr_idx], y[va_idx]\n\n            cb = CatBoostRegressor(\n                loss_function=\"RMSE\",\n                iterations=30000,\n                learning_rate=0.03,\n                depth=8,\n                l2_leaf_reg=6,\n                random_seed=SEED,\n                verbose=1000,\n                od_type=\"Iter\",\n                od_wait=800,\n            )\n            cb.fit(X_tr, y_tr, eval_set=(X_va, y_va), use_best_model=True)\n\n            va_pred = cb.predict(X_va)\n            fold_rmse = rmse(y_va, va_pred)\n            fold_scores_cb.append(fold_rmse)\n\n            pred_test_cb += cb.predict(test_fe) / N_SPLITS\n\n            print(f\"[CB] Fold {fold}: RMSE={fold_rmse:.6f}\")\n\n            del cb, X_tr, X_va, y_tr, y_va\n            gc.collect()\n\n        print(\"\\n[CB] Mean:\", np.mean(fold_scores_cb), \"Std:\", np.std(fold_scores_cb))\n\n        # Blend weights\n        W_LGB = 0.6\n        W_CB  = 0.4\n        final_pred = W_LGB * pred_test_lgb + W_CB * pred_test_cb\n        print(f\"\\nBlending: LGB={W_LGB}  CB={W_CB}\")\n\n# =========================================================\n# ðŸ˜Ž Create submission.csv\n# =========================================================\nsub2 = sub.copy()\n\n# Find submission target column (not ID)\nsub_target = [c for c in sub2.columns if c.lower() not in [\"id\", \"index\"]]\nif len(sub_target) != 1:\n    print(\"Submission columns:\", sub2.columns.tolist())\n    raise ValueError(\"Could not infer submission target column.\")\nsub_target = sub_target[0]\n\nsub2[sub_target] = final_pred\nsub2.to_csv(\"submission.csv\", index=False)\n\nprint(\"\\nSaved submission.csv âœ…\")\ndisplay(sub2.head())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-14T11:42:17.274865Z","iopub.execute_input":"2026-02-14T11:42:17.275544Z"}},"outputs":[{"name":"stdout","text":"train: (376387, 21) | test: (185385, 21) | sub: (185385, 2)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   MonsoonIntensity  TopographyDrainage  RiverManagement  Deforestation  \\\n0         10.907827            3.450074         4.031750       4.849948   \n1               NaN            4.923656              NaN      35.602568   \n2          7.036374            9.198907         7.335568       7.727015   \n3          5.853376            8.221989         6.968215       3.617405   \n4          5.883916            2.269436         5.460944       5.848432   \n\n   Urbanization  ClimateChange  DamsQuality  Siltation  AgriculturalPractices  \\\n0      9.671641       4.934358          NaN   5.719451               4.197449   \n1      8.867987       5.133364     3.503824   2.413313               1.978167   \n2      5.059921       1.875038     2.785596   5.246909               7.119318   \n3           NaN       2.236813     3.315606        NaN               8.114619   \n4      6.754861            NaN     8.535180   4.219204               5.938841   \n\n   Encroachments  ...  DrainageSystems  CoastalVulnerability  Landslides  \\\n0       5.450808  ...         1.826199                   NaN    2.100308   \n1       8.525660  ...         4.946545              7.822959    3.352290   \n2       7.838039  ...         2.566081              3.892975    6.976857   \n3       6.212545  ...              NaN              3.090245    5.943337   \n4       2.573560  ...         5.912197              5.859797    6.334664   \n\n   Watersheds  DeterioratingInfrastructure  PopulationScore  WetlandLoss  \\\n0    4.781873                     3.550904         6.009351     5.178502   \n1    3.887192                     3.686099              NaN     9.037464   \n2   38.595509                     5.927815         8.026921     1.947679   \n3    3.259453                     4.208167         3.250709     2.405658   \n4    4.076011                     8.260247         3.315948     3.834493   \n\n   InadequatePlanning  PoliticalFactors  FloodProbability  \n0            1.737822          5.820953             0.500  \n1            3.950068               NaN             0.545  \n2            3.335545          4.717355             0.545  \n3            4.857694          6.087312             0.500  \n4            3.291773          4.810802             0.545  \n\n[5 rows x 21 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MonsoonIntensity</th>\n      <th>TopographyDrainage</th>\n      <th>RiverManagement</th>\n      <th>Deforestation</th>\n      <th>Urbanization</th>\n      <th>ClimateChange</th>\n      <th>DamsQuality</th>\n      <th>Siltation</th>\n      <th>AgriculturalPractices</th>\n      <th>Encroachments</th>\n      <th>...</th>\n      <th>DrainageSystems</th>\n      <th>CoastalVulnerability</th>\n      <th>Landslides</th>\n      <th>Watersheds</th>\n      <th>DeterioratingInfrastructure</th>\n      <th>PopulationScore</th>\n      <th>WetlandLoss</th>\n      <th>InadequatePlanning</th>\n      <th>PoliticalFactors</th>\n      <th>FloodProbability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10.907827</td>\n      <td>3.450074</td>\n      <td>4.031750</td>\n      <td>4.849948</td>\n      <td>9.671641</td>\n      <td>4.934358</td>\n      <td>NaN</td>\n      <td>5.719451</td>\n      <td>4.197449</td>\n      <td>5.450808</td>\n      <td>...</td>\n      <td>1.826199</td>\n      <td>NaN</td>\n      <td>2.100308</td>\n      <td>4.781873</td>\n      <td>3.550904</td>\n      <td>6.009351</td>\n      <td>5.178502</td>\n      <td>1.737822</td>\n      <td>5.820953</td>\n      <td>0.500</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>4.923656</td>\n      <td>NaN</td>\n      <td>35.602568</td>\n      <td>8.867987</td>\n      <td>5.133364</td>\n      <td>3.503824</td>\n      <td>2.413313</td>\n      <td>1.978167</td>\n      <td>8.525660</td>\n      <td>...</td>\n      <td>4.946545</td>\n      <td>7.822959</td>\n      <td>3.352290</td>\n      <td>3.887192</td>\n      <td>3.686099</td>\n      <td>NaN</td>\n      <td>9.037464</td>\n      <td>3.950068</td>\n      <td>NaN</td>\n      <td>0.545</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7.036374</td>\n      <td>9.198907</td>\n      <td>7.335568</td>\n      <td>7.727015</td>\n      <td>5.059921</td>\n      <td>1.875038</td>\n      <td>2.785596</td>\n      <td>5.246909</td>\n      <td>7.119318</td>\n      <td>7.838039</td>\n      <td>...</td>\n      <td>2.566081</td>\n      <td>3.892975</td>\n      <td>6.976857</td>\n      <td>38.595509</td>\n      <td>5.927815</td>\n      <td>8.026921</td>\n      <td>1.947679</td>\n      <td>3.335545</td>\n      <td>4.717355</td>\n      <td>0.545</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5.853376</td>\n      <td>8.221989</td>\n      <td>6.968215</td>\n      <td>3.617405</td>\n      <td>NaN</td>\n      <td>2.236813</td>\n      <td>3.315606</td>\n      <td>NaN</td>\n      <td>8.114619</td>\n      <td>6.212545</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>3.090245</td>\n      <td>5.943337</td>\n      <td>3.259453</td>\n      <td>4.208167</td>\n      <td>3.250709</td>\n      <td>2.405658</td>\n      <td>4.857694</td>\n      <td>6.087312</td>\n      <td>0.500</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.883916</td>\n      <td>2.269436</td>\n      <td>5.460944</td>\n      <td>5.848432</td>\n      <td>6.754861</td>\n      <td>NaN</td>\n      <td>8.535180</td>\n      <td>4.219204</td>\n      <td>5.938841</td>\n      <td>2.573560</td>\n      <td>...</td>\n      <td>5.912197</td>\n      <td>5.859797</td>\n      <td>6.334664</td>\n      <td>4.076011</td>\n      <td>8.260247</td>\n      <td>3.315948</td>\n      <td>3.834493</td>\n      <td>3.291773</td>\n      <td>4.810802</td>\n      <td>0.545</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 21 columns</p>\n</div>"},"metadata":{}},{"name":"stdout","text":"ID_COL: None\n\nMissing values:\nTrain missing: 565726\nTest missing : 278802\n\nTarget stats:\ncount    376387.000000\nmean          0.504375\nstd           0.051012\nmin           0.285000\n25%           0.470000\n50%           0.505000\n75%           0.540000\nmax           0.725000\ndtype: float64\n\nAfter FE:\nX_fe: (376387, 25) test_fe: (185385, 25)\n\n===== LightGBM CV =====\n[500]\tvalid's rmse: 0.0267246\n[1000]\tvalid's rmse: 0.0267127\nFold 1: RMSE=0.026708 | best_iter=808\n[500]\tvalid's rmse: 0.0266136\n[1000]\tvalid's rmse: 0.0266015\nFold 2: RMSE=0.026599 | best_iter=760\n","output_type":"stream"}],"execution_count":null}]}